{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431c3b6-91a7-4c07-906b-32acf3144f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "NB_DIR = os.path.abspath('')\n",
    "\n",
    "# To make nbdir import-able\n",
    "if NB_DIR not in sys.path:\n",
    "    sys.path.append(NB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda510b-4990-484a-a759-26db11aba5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n",
    "    GradientBoostingClassifier, HistGradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, \\\n",
    "    VotingClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Our own libraries ---\n",
    "\n",
    "import feature_extraction as fe\n",
    "\n",
    "# Reload automatically upon any change in feature_extraction.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d984e914-e220-4165-be75-3c2b4e7bae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from file into train_data list\n",
    "\n",
    "train_file_path = os.path.join(NB_DIR, 'data', 'train.jsonl')\n",
    "train_data = []\n",
    "\n",
    "# Read and decode the file line by line\n",
    "print(f\"Loading data from '{train_file_path}'...\")\n",
    "try:\n",
    "    with open(train_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # json.loads() parses one line (one JSON object) into a Python dictionary\n",
    "            train_data.append(json.loads(line))\n",
    "    print(f'Successfully loaded {len(train_data)} battles.')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find the training file at '{train_file_path}'.\")\n",
    "    print('Please make sure you have added the competition data to this notebook.')\n",
    "\n",
    "except IOError:\n",
    "    print(f\"An error occurred while reading the file '{train_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8b3aa-d1fe-42cf-8ea9-4d0599f6ee25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Sneak a peek into the just loaded battle data.\n",
    "\n",
    "print(\"\\n--- Structure of the first train battle: ---\")\n",
    "if train_data:\n",
    "    first_battle = train_data[0]\n",
    "    \n",
    "    # To keep the output clean, we can create a copy and truncate the timeline\n",
    "    battle_for_display = first_battle.copy()\n",
    "    battle_for_display['battle_timeline'] = battle_for_display.get('battle_timeline', [])[:5]\n",
    "    \n",
    "    # Use json.dumps for pretty-printing the dictionary\n",
    "    print(json.dumps(battle_for_display, indent=4))\n",
    "    if len(first_battle.get('battle_timeline', [])) > 5:\n",
    "        print(\"    ...\")\n",
    "        print(\"    [battle_timeline has been truncated for display]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf099be-7f39-4615-a3d6-eaa3e52ecf6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "At this point we have all the training data, decoded from JSON, in the `train_data` list (of nested structures), so it's time to work on the features, through techniques such as features regularization and selection. All the relevant functions are in the `feature_extraction` library (file `feature_extraction.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7edb6-13fd-4101-9510-643dc559d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing training data...\")\n",
    "train_df = fe.extract_full_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf3085-3c3f-4466-8875-3141a8f7aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sneak a peek into the features dataframe.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068240b-527f-4d8d-8b3c-9ce5babe0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: delete bogus line\n",
    "# https://classroom.google.com/c/MjM1MTYxMzEyMTda/p/ODE1OTEyMTU1OTM3/details?hl=it\n",
    "\n",
    "train_df.drop(train_df.index[[4877]], inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedbf3b-a6f4-49b5-94b3-552820300daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: sneak a peek into text columns.\n",
    "text_cols = [col for col in train_df.columns if 'name' in col or 'type' in col]\n",
    "train_df[text_cols].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da0271-586a-472b-9384-c97facb4f406",
   "metadata": {},
   "source": [
    "# Models Training and Comparison\n",
    "\n",
    "At this point we have all the selected features in to the `train_df` dataframe, so it's time to train the various models on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751d65c-1618-431f-8c03-a50974c0d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all the text columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "text_cols = [col for col in train_df.columns if 'name' in col or 'type' in col]\n",
    "for col in text_cols: \n",
    "    train_df[col] = le.fit_transform(train_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c08e0f-083a-44aa-9d12-0f5e05373283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our features (X) and target (y)\n",
    "features = [col for col in train_df.columns if col not in ['battle_id', 'player_won']]\n",
    "\n",
    "# Any data scaling/regularization goes here\n",
    "scaler = StandardScaler().fit(train_df[features])\n",
    "train_df_scaled = scaler.transform(train_df[features])\n",
    "\n",
    "# Split the data (and decide whether to use scaling or not).\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df_scaled, train_df['player_won'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4544f-2bb1-464c-b1c4-9f4d88c35fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (optional)\n",
    "pca = PCA(n_components=30)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55c258-edd1-46bc-ae38-9c0308e86509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models list (just add models with their parameters!)\n",
    "models = [\n",
    "    #LogisticRegression(max_iter=10_000),\n",
    "    #LogisticRegressionCV(max_iter=10_000),\n",
    "    #SGDClassifier(max_iter=10_000, tol=1e-3),\n",
    "    #GaussianNB(),\n",
    "    #DecisionTreeClassifier(random_state=0, criterion='entropy', max_depth=5),\n",
    "    #GradientBoostingClassifier(),\n",
    "    HistGradientBoostingClassifier(max_iter=10_000),\n",
    "    #BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5),\n",
    "    #ExtraTreesClassifier(n_estimators=100, random_state=0),\n",
    "    #LinearSVC(random_state=0,dual=False),\n",
    "    #RandomForestClassifier(n_estimators=100),\n",
    "    #KNeighborsClassifier(n_neighbors=20),\n",
    "    #GaussianProcessClassifier(),\n",
    "    #AdaBoostClassifier(),\n",
    "    #MLPClassifier(max_iter=10_000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b673d5-2a36-4cf7-834f-05015e372c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing with normal training\n",
    "models_result = []\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    models_result.append([model.__class__.__name__, model.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e813d54-4998-47cf-b0b0-53df6d54603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing with cross-validation training - Alternative to the above!\n",
    "models_result = []\n",
    "\n",
    "for model in models:\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "    cv_results = cross_val_score(model, train_df_scaled, train_df['player_won'], cv=cv)\n",
    "    print(f'{model.__class__.__name__:<32} mean: {cv_results.mean():.3f}\\tmin: {cv_results.min():.3f}\\tmax: {cv_results.max():.3f}')\n",
    "    models_result.append([model.__class__.__name__, cv_results.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88785f91-84a6-4e80-936b-f84affe76958",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = PrettyTable()\n",
    "results_table.field_names = ['Model Name', 'Accuracy']\n",
    "results_table.align['Model Name'] = 'r'\n",
    "results_table.align['Accuracy'] = 'l'\n",
    "results_table.add_rows(sorted([[result[0], round(result[1]*100, 3)] for result in models_result], key=lambda row: row[1]))\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70418c8-bb6f-43a1-850f-e285da6257f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = HistGradientBoostingClassifier(max_iter=10_000)\n",
    "clf2 = LogisticRegressionCV()\n",
    "clf3 = AdaBoostClassifier()\n",
    "clf4 = RandomForestClassifier()\n",
    "clf5 = DecisionTreeClassifier()\n",
    "\n",
    "estimators=[('HGBC', clf1), ('GBC', clf2), ('A', clf3), ('B', clf4), ('C', clf5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dedac8-52a9-4044-a65b-4f6501a076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE - Voting\n",
    "eclf = VotingClassifier(estimators=estimators, voting='hard')\n",
    "eclf.fit(X_train, y_train)\n",
    "eclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f141c-6f4a-474c-b85a-be8d13807cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE - Stacking\n",
    "eclf = StackingClassifier(estimators=estimators, final_estimator=HistGradientBoostingClassifier())\n",
    "eclf.fit(X_train, y_train)\n",
    "eclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64f72d-a6b9-405e-8f2d-de689861a9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
